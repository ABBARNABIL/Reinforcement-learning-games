{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#define the shape of the environment (i.e., its states)\n",
    "environment_shape = 6\n",
    "\n",
    "\n",
    "#Create a 2D numpy array to hold the current Q-values for each state and action pair: Q(s, a)\n",
    "#each state (see next cell for a description of possible actions).\n",
    "#The value of each (state, action) pair is initialized to 0.\n",
    "q_values = np.zeros((environment_shape,2))\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#define actions\n",
    "#numeric action codes: 0 = right , 1 = left\n",
    "actions = ['right','left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.   -0.01 -0.01 -0.01 -0.01  1.  ]\n"
     ]
    }
   ],
   "source": [
    "#Create a 1D numpy array to hold the rewards for each state.\n",
    "rewards = np.full(6, -0.01)\n",
    "rewards[0] = -1. #set the reward for trap to -1\n",
    "rewards[5] = 1. #set the reward for goal to 1\n",
    "\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_index):\n",
    "  if rewards[current_index] == -0.01:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "\n",
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "def get_next_action(current_index, epsilon):\n",
    "  #if a randomly chosen value between 0 and 1 is less than epsilon,\n",
    "  #then choose the most promising value from the Q-table for this state.\n",
    "  if np.random.random() < epsilon:\n",
    "    return np.argmax(q_values[current_index])\n",
    "  else: #choose a random action\n",
    "    return np.random.randint(2)\n",
    "\n",
    "#define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_index, action_index):\n",
    "  new_index = current_index\n",
    "  if actions[action_index] == 'right' and current_index > 0:\n",
    "    new_index -= 1\n",
    "  elif actions[action_index] == 'left' and current_index < 5:\n",
    "    new_index += 1\n",
    "  return new_index\n",
    "\n",
    "\n",
    "def get_shortest_path(start_player_index):\n",
    "  if is_terminal_state(start_player_index):\n",
    "    return []\n",
    "  else:\n",
    "    current_player_index = start_player_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_player_index])\n",
    "    while not is_terminal_state(current_player_index):\n",
    "      action_index = get_next_action( current_player_index, 1.)\n",
    "      current_player_index = get_next_location(current_player_index, action_index)\n",
    "      shortest_path.append([current_player_index])\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.       0.     ]\n",
      " [-0.9999   0.7019 ]\n",
      " [ 0.62171  0.791  ]\n",
      " [ 0.7019   0.89   ]\n",
      " [ 0.791    1.     ]\n",
      " [ 0.       0.     ]]\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "\n",
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "  #get the starting location for this episode\n",
    "  player_index = 2\n",
    "\n",
    "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "  while not is_terminal_state(player_index):\n",
    "    #choose which action to take (i.e., where to move next)\n",
    "    action_index = get_next_action(player_index, epsilon)\n",
    "\n",
    "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "    old_player_index = player_index #store the old row and column indexes\n",
    "    player_index = get_next_location(player_index, action_index)\n",
    "\n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[player_index]\n",
    "    old_q_value = q_values[old_player_index, action_index]\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[player_index])) - old_q_value\n",
    "\n",
    "    #update the Q-value for the previous state and action pair\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_player_index, action_index] = new_q_value\n",
    "print(q_values)\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2], [3], [4], [5]]\n"
     ]
    }
   ],
   "source": [
    "#example of location to test the behavior\n",
    "print(get_shortest_path(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the Q-values\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(q_values, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2D Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "#define the shape of the environment (i.e., its states)\n",
    "environment_rows = 3\n",
    "environment_columns = 4\n",
    "\n",
    "\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))\n",
    "\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04 -0.04 -0.04  1.  ]\n",
      " [-0.04 -1.   -0.04 -1.  ]\n",
      " [-0.04 -0.04 -0.04 -0.04]]\n"
     ]
    }
   ],
   "source": [
    "#Create a 2D numpy array to hold the rewards for each state.\n",
    "rewards = np.full((environment_rows, environment_columns), -0.04)\n",
    "rewards[0, 3] = 1. #set the reward for the goal (i.e., the goal) to 1\n",
    "rewards[1, 3] = -1. #set the reward for the trap to -1\n",
    "rewards[1,1] = -1. #set the reward for the block to -1\n",
    "\n",
    "#print rewards matrix\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  if rewards[current_row_index, current_column_index] == -0.04:\n",
    "    return False\n",
    "  else:\n",
    "    return True\n",
    "\n",
    "\n",
    "def set_starting_location():\n",
    "  return 2, 0\n",
    "\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "  if np.random.random() < epsilon:\n",
    "    return np.argmax(q_values[current_row_index, current_column_index])\n",
    "  else:\n",
    "    return np.random.randint(4)\n",
    "\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "  new_row_index = current_row_index\n",
    "  new_column_index = current_column_index\n",
    "  if actions[action_index] == 'up' and current_row_index > 0:\n",
    "    new_row_index -= 1\n",
    "  elif actions[action_index] == 'right' and current_column_index > 0:\n",
    "    new_column_index -= 1\n",
    "  elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "    new_row_index += 1\n",
    "  elif actions[action_index] == 'left' and current_column_index < environment_columns - 1:\n",
    "    new_column_index += 1\n",
    "  return new_row_index, new_column_index\n",
    "\n",
    "\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  if is_terminal_state(start_row_index, start_column_index):\n",
    "    return []\n",
    "  else:\n",
    "    current_row_index, current_column_index = start_row_index, start_column_index\n",
    "    shortest_path = []\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    while not is_terminal_state(current_row_index, current_column_index):\n",
    "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "    return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.62059993  0.62059926  0.51791761  0.734     ]\n",
      "  [ 0.73399197  0.6206     -0.999       0.86      ]\n",
      "  [ 0.86        0.734       0.734       1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.6206      0.51853936  0.42663102 -0.99999999]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.86       -1.          0.6206     -1.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.51854     0.426686    0.426686    0.51854   ]\n",
      "  [-1.          0.426686    0.51854     0.6206    ]\n",
      "  [ 0.734       0.51854     0.6206      0.51854   ]\n",
      "  [-0.9         0.6206      0.45411593 -0.12570016]]]\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#define training parameters\n",
    "epsilon = 0.9 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "\n",
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "  #get the starting location for this episode\n",
    "  row_index, column_index = set_starting_location()\n",
    "\n",
    "  #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "  while not is_terminal_state(row_index, column_index):\n",
    "    #choose which action to take (i.e., where to move next)\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "    #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[row_index, column_index]\n",
    "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "    #update the Q-value for the previous state and action pair\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "print(q_values)\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0], [1, 0], [0, 0], [0, 1], [0, 2], [0, 3]]\n",
      "[[2, 3], [2, 2], [1, 2], [0, 2], [0, 3]]\n"
     ]
    }
   ],
   "source": [
    "#example of location to test the behavior\n",
    "print(get_shortest_path(2, 0))\n",
    "print(get_shortest_path(2, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}